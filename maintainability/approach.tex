% !TEX root =  paper.tex


\section{Proposed Approach}
\label{section:approach}


\Cref{fig:approach-diagram} shows an overview of our proposed approach
to automatically generate modularized reusable UI components
from mockups.
The approach begins by retrieving the DOM of the web app's mockup. 
Next, a visual abstraction is performed to generate a normalized and abstract representation of the web app's UI layout.
This transforms the mockup into a set of {\VizElem}s ({\VE}s) on which further analysis is conducted. 
The approach then performs a dynamic grouping  of {\VizElem}s,
to identify subtrees
which correspond to potential instances of a UI component. 
This grouping is used in the next step, where an unsupervised machine learning technique applied on the 
potential UI component instances identifies UI components.
Finally, the actual code for the UI components is generated by refactoring the original \html code.

\begin{figure}
    \centering
    \includegraphics[width=0.75\linewidth]{maintainability/figures/approach-diagram}
    \caption{Overview of the proposed approach.} 
    \label{fig:approach-diagram}
\end{figure} 


In the following subsections, we describe each step of the proposed approach and illustrate some of their major components and analysis procedures. 

\subsection{Definitions}
Before we proceed to describe the details of the proposed approach, we begin by declaring a few important definitions that are used throughout the chapter.
\begin{defn}[\textbf{UI Component}]
A UI component $c_E=\langle n, N \rangle$ 
for a repeated group of UI element trees $E$ in a web application
is a tree structure rooted at $n \in N$, where $N= T \cup P$ is a set of abstract user interface elements.
The component includes the \emph{template} $T$ and the \emph{placeholders} $P$.
The template of the UI component denotes the nodes which do not change wherever the component is used (i.e., \emph{instantiated}),
while the placeholders captures the changed nodes, whether partially or fully changed, as explained in \Cref{sec:ui-comp-gen}.
\end{defn} 
In this chapter, we use the terms \textit{UI component} and \textit{component} interchangeably.


\begin{defn}[\textbf{Component Instance}]
A component instance $i=\langle c_E, f \rangle$ is a concrete and specific instantiation of a UI component $c_E$. 
Component instances share the template part with other instances of the same component, 
but differ in the placeholder parts.
The function $f: P \rightarrow V$ assigns values $v \in V$ to the placeholders $p \in P$ of $c_E$.
\end{defn}

\begin{defn}[\textbf{Potential Instance}]
A potential instance is a subtree of the \dom 
constructed for a web application's user interface,
representing a concrete UI element tree 
that is \emph{likely} to form a component instance, but may not be so. 
\end{defn}

Potential instances are processed at multiple stages of the proposed approach 
until they are either discarded or associated with a component.


\subsection{Visual UI Normalization}
In the first step of the approach, we take as input the DOM of the mockup after it is loaded and rendered in a browser,
and perform a \emph{visual normalization} that transforms the DOM into a set of \emph{{\VizElem}s}.
The goal of this step is to normalize the visual presentation of a web user interface into a set of abstract elements 
that signify the salient features of the page from a visual perspective, which may represent potential component instances.
The intuition behind this is that normalization and abstraction can be helpful to achieve our goal of detecting reusable patterns, since the exact and minute details are less relevant when identifying repeated regions of a web page. 
Furthermore, component instances are generally different from each other in some aspects, while they still have similar overall visual appearance. 
This normalization step enables obtaining a big picture to identify these potential similarities.% between different instances.

The visual normalization is achieved as follows.
First, we extract from the DOM a set of nodes that represent visual content of the UI, and we refer to each of these as \emph{{\VizElem}s}.
We define two main types of {\VizElem}s: textual and graphical (image).
The extraction of text content is achieved by traversing text nodes of the DOM. More specifically:
%\begin{align}
%VE_{T} \coloneqq \{ node \in DOM \!\, : \, & node.type=\code{\#TEXT} \nonumber \\
%                       \lor \, & node.tag=\code{input} \}
%\end{align}
\begin{align}
\Gamma_{T} \coloneqq \{ E(node) : \ & node \in DOM_R \!\, 			\land \,  & node.hasTextContent \}
\end{align}
where $\Gamma_{T}$ is the set of all visual elements that represent text in the UI, $DOM_R$ is the rendered DOM in the web browser, and $E(node)$ maps the node to the corresponding element. 
%\davood{visual element?}
The predicate $hasTextContent$ examines whether there is a text associated with the node, 
and covers two possibilities: non-empty nodes of type \code{\#TEXT}, representing string literals in $DOM_R$, 
and nodes of \code{input} elements that have an associated text value (e.g., buttons or lists).
Subsequently, we perform another extraction for image content. We define this as follows:

%\begin{align}
%VE_{I} \coloneqq \{ & node \in DOM \!\, : \, node.tag=\code{img} \nonumber \\
%                       \lor \, & node.hasBackgroundImage \}
%\end{align}
\begin{align}
\Gamma_{I} \coloneqq \{ E(node) : \ & node \in DOM_R \!\, 		\land \, & node.hasImageContent \}
\end{align}
where $\Gamma_{I}$ is the set of all visual elements that represent images. As in the previous case, the predicate $hasImageContent$ examines if there is an image associated with the node. This again has two possibilities: nodes of \code{<img>} elements and non-img nodes with a non-null background image attribute. 

Subsequently, we use the set of all {\VizElem}s to construct the normalized UI: 
\begin{align}
\mathrm{UI}_N = V\!\left( \Gamma_I \cup \Gamma_T \right)
\end{align}
where $\mathrm{UI}_N$ is the resultant normalized UI and $V$ is a visual projection operation that generates an image from the union of visual elements. This is achieved as follows. First, we begin by collecting the final \emph{computed} properties of each element,
when rendered in the web browser. These properties represent the final state of elements after the propagation of all changes and events. The properties we collect are the size, location, and z-orders of these elements. Next, we assign different colors to each class of {\VizElem}s.
We assign green for elements in $\Gamma_{T}$, and blue for elements in $\Gamma_{I}$.
While any arbitrary colors could have been chosen, we chose these two colors in order to facilitate faster visual analysis in subsequent steps, since these two  are typically represented in separate color channels.
\Cref{fig:example-normalization} illustrates an example of the output generated from this visual normalization step.
As can be observed, the minute details of the page are abstracted away while the main and essential structure of the UI is accentuated. 

\begin{figure}
    \centering
    \includegraphics[height=3.5cm,width=0.40\textwidth, clip, trim={0 0cm 0 0cm}]{maintainability/figures/example-normalization}
    \caption{The result of the visual UI normalization stage (as applied to the motivating example of \Cref{fig:motivating-example}). Best viewed on a color display.}
    \label{fig:example-normalization}
\end{figure} 

\subsection{Potential Instance Identification}
The result of the previous step consists of only a set of {\VizElem}s.
These {\VizElem}s on their own do not \emph{necessarily} represent reusable repetitive UI patterns. 
The goal of this step is to transform the set of individual {\VizElem}s into a set of \emph{potential} reusable UI component instances.
A potential instance is a region of the UI that has been determined to be likely repeated somewhere else in the UI. That is, it is 
a region where, using all of the aforementioned steps, the 
initial analysis have indicated that this instance would likely be a good candidate to be combined with other repetitions and refactored 
into a single component. These potential component instances will be further checked and analyzed in the subsequent steps in order to generate a final set of components.

Identifying potential component instances can be an intricate decision since there are multiple levels of hierarchy that can be considered.
For example, consider group \circled{A} in \Cref{fig:motivating-example}.
Notice how the icons in that group would constitute repeated elements.
The same is true for the text labels under the icons.
Yet another repetition pattern is taking the icon and text as one component that is repeated multiple times. 
Accordingly, in order to identify potential component instances, we propose an approach that aims to maximize two complementary aspects, namely, 
the number of repetitions of a component,
and the amount of repetitions encapsulated \emph{within} each component instance (i.e.,~repetitions of the \emph{same} potential instances).
We refer to this combination of aspects as the \emph{modularization potential}, where a high value of modularization potential indicates a potentially more reusable UI component.
Our goal is therefore to utilize this modularization potential to optimize a set of potential instances, $\Psi$:
% capture these aspects in a quantitative manner and optimize for the \emph{maximal modularization component set}, $MMCS$:
%\begin{equation} \label{eqn:MMCS}
%\Psi \coloneqq \argmax_{C} \prod_{c_i \in C} \big\lvert \{VE_T \cup VE_I  : VE_T, VE_I \in c_i\} \big\rvert
%\end{equation}
\begin{equation} \label{eqn:MMCS}
\Psi \coloneqq \argmax_{C} \prod_{c_i \in C} \big\lvert \{c_i(\gamma_T, \gamma_I) : \gamma_T, \gamma_I \subset \mathrm{UI}_N\} \big\rvert
\end{equation}
where $C$ is the set of all component instances, $c_i$ is a potential instance, and the optimized function is the modularization potential.
This optimization yields a global optimum set of potential instances between two extremes.
At one end of the spectrum,
each {\VizElem} represents a component of its own.
This yields a sub-optimal component set that has low modularization potential because of a lack of repetitions.
For this case, the modularization potential in \cref{eqn:MMCS} yields a score of 1 since each component encapsulates only a single element.
At the other end of the spectrum,
one might theoretically consider the \emph{entire} collection of {\VizElem}s to represent a single component that is repeated only once.
This results in a score equal to $N$, the number of total {\VizElem}s, in \cref{eqn:MMCS}.
$\Psi$, on the other hand, represents a global optimum between the aforementioned two extremes. $\Psi$ captures a set of potential component instances that aims for \emph{both} a large number of components, and for an instance that in itself has a large number of UI repetitions.
The subsequent steps of the approach will therefore only use $\Psi$ for further analyses and final generation of components.

We now describe the implementation for generating $\Psi$. \Cref{fig:instance-potential-identification} shows an illustration of this process.
First, we obtain DOM locators (e.g., {\xpath} expressions) for each of the {\VizElem}s. Next, starting from these locators as leaf nodes, we iteratively build a tree from the bottom up (as shown in \Cref{fig:instance-potential-identification}), adding the DOM parent of every tree node with each iteration.
At each iteration, we calculate the modularizaton potential of \cref{eqn:MMCS}, with every node's subtree representing a potential instance $c_i$. 
The potential instances are illustrated using the red outlines in \Cref{fig:instance-potential-identification}. Note how at the very first iteration, each potential instance is simply the \VizElem~itself. In the next iteration, the potential instances grow larger to include more {\VizElem}s as shown by the larger red outlines at iteration 1.
Finally, the iteration that yields the maximal modularization is  reported as the $\Psi$ set and passed to the subsequent stage.

\begin{figure}
    \centering
    \includegraphics[width=0.9\linewidth]{maintainability/figures/potential-component-identification}
    \caption{Illustration of the potential instance identification stage. Each iteration considers a different group of potential instances before selecting an optimum set.}
    \label{fig:instance-potential-identification}
\end{figure} 

\subsection{Unsupervised Visual Matching}
The output of the previous step is a set of \emph{potential} component instances that maximizes the modularization potential out of many alternative sets of instances.
However, these are only \emph{potential} instances that may or may not actually belong to a component.
In other words, there is still no information as to which subgroup of potential instances do indeed belong together and constitute a reusable component,
versus other potential instances that are simply visual elements that do not represent repetitive reusable components.
In this stage, we process the set of \emph{potential} component instances and reduce it into a final set of components. 

In order to create the final components,
we propose an approach that visually examines potential instances and combines them into components via unsupervised machine learning.
The intuition behind adopting this approach is that if potential instances match with other potential instances,
the ``potential'' qualifier can be dropped from these instances and they would be recognized as constituting a component together.
In this approach, we use a clustering mechanism to create components in order to facilitate robust matching of potential instances.

We now describe the details of the process. First, we obtain the screenshot image of the {\VizElem}s per potential instance. This results in one image (containing all {\VizElem}s) for each potential instance. Next, for each potential instance image, we extract a feature vector. We compute the feature vector using a \emph{vectorized pixel histogram}, which is a process that captures a summary of the overall content in the instance image. However, unlike typical approaches from the machine vision literature~\cite{lopes2010automatic, liu2010image} where a binning parameter (a parameter for categorizing pixels) is required, we generate the vectorized histogram without requiring this parameter. Instead, due to the nature of visual normalization that we have proposed, only two categories need to be considered: one for text visual elements, and another for image visual elements. Therefore, we finally end up with a feature vector for each potential instance. Subsequently, we compute the cosine distance between each pair $\mathbf{I}_i$, $\mathbf{I}_j$ of potential instances:
\begin{equation} \label{eqn:cosdist}
D_{i,j} = 1 - \frac{\mathbf{I}_i \cdot \mathbf{I}_j}{ \lVert \mathbf{I}_i \rVert \lVert \mathbf{I}_j \rVert }
\end{equation}

Next, we perform an unsupervised clustering process. The selection of an appropriate clustering is of paramount importance due to a couple of challenges. First, the clustering can be challenging due to the wide range of possibilities of arrangements and structures of component instances. In other words, there is potentially a large range of \emph{inter-} and \emph{intra-}component variations. 
This makes it difficult to use hierarchical clustering, for instance, due to its very high sensitivity to outliers and therefore would be a poor choice for handling large component variations, and also due to its high dependence on order of data, which can make it less effective for detecting instances far way from each other. Furthermore, performing a cut on the clustered hierarchies often requires specifying the number of clusters or some other parameter, which can be difficult and brittle to specify. Density-based algorithms (e.g., DBSCAN) would not be effective either, as they would have difficulty handling the \emph{variable} densities present between potential clusters of instances. Accordingly, we opted for a technique that can be flexible enough to correctly identify such variations and be able to better recognize the final components. To do this, we select a method that performs variable-density clustering with a hierarchy of densities \cite{campello2013density}. The hierarchy of variable-densities allows the method to automatically detect stable clusters in a parameter-free fashion. More importantly, the method is built to handle varying-densities, which becomes very important when handling the potentially large range of inter- and intra-component variations.
 
Once the components have been identified through unsupervised visual matching,
we extract the corresponding locator in the DOM (e.g., {\xpath}s) per instance.
The final result is a superset of component instance locator sets.
This superset is passed on to the next step in order to combine the component instances into final components.


\subsection{UI Component Generation}
\label{sec:ui-comp-gen}
\input{maintainability/ui-component-generation}

\header{Implementation}
\label{section:implementation}

We implemented the proposed approach in a tool called \toolname~\cite{tool-and-data} (short for \textbf{Vis}ual \textbf{Mod}ularizer). 
\toolname is implemented in Java and Python~3. We use the Selenium web driver to view the mockup and extract DOM trees and their relevant computed properties. 
For clustering, we use the implementation provided by Campello et. al.~\cite{campello2013density} 
and the \code{numpy}~\cite{walt2011numpy} library for mathematical and numerical functions. 




 
